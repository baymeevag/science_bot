{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os, shutil, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# establishing session\n",
    "s = requests.Session()\n",
    "\n",
    "def load_page(url, session):\n",
    "    r = session.get(url)\n",
    "    encoding = r.encoding if 'charset' in r.headers.get('content-type', '').lower() else None\n",
    "    soup = BeautifulSoup(r.content, 'lxml', from_encoding=encoding)\n",
    "    return soup\n",
    "    \n",
    "def clear_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    for the_file in os.listdir(path):\n",
    "        file_path = os.path.join(path, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "clear_dir('./cyberleninka_all/')\n",
    "page = 1\n",
    "k = 0\n",
    "while k < 10:\n",
    "    url = 'http://cyberleninka.ru/article/c/matematika/%d' % (page)\n",
    "    soup = load_page(url, s)\n",
    "    if soup:\n",
    "        article_list = soup.findAll('div', {'class': 'title'})\n",
    "        for a in article_list:\n",
    "            english = r'[A-Z|a-z]+$'\n",
    "            bad_chars = r'(\\w)*(\\{[^}]+)|(\\$)+|(\\\\)+|(\\/)+(\\w)*'\n",
    "            garbage = r'[.jpg]+|[.png]+|[.bmp]+|[.pdf]+'\n",
    "            if re.search(english, a.text) or re.search(bad_chars, a.text) or re.search(garbage, a.text):\n",
    "                continue\n",
    "            with open('./cyberleninka_all/page_%d.html' % (page), 'a') as output_file:\n",
    "                output_file.write(a.text.encode('utf8') + '\\n')\n",
    "    else:\n",
    "        break\n",
    "    page += 1\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named dryscrape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-fcc809347482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mdryscrape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_search_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://cyberleninka.ru/search#q=%s&page=%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named dryscrape"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import dryscrape\n",
    "\n",
    "def load_search_results(query, page, session):\n",
    "    url = 'http://cyberleninka.ru/search#q=%s&page=%d' % (urllib.quote(query), page)\n",
    "    r = session.get(url)\n",
    "    encoding = r.encoding if 'charset' in r.headers.get('content-type', '').lower() else None\n",
    "    soup = BeautifulSoup(r.content, 'lxml', from_encoding=encoding)\n",
    "    return soup\n",
    "\n",
    "query = 'виртуальная реальность'\n",
    "page = 1\n",
    "url = 'http://cyberleninka.ru/search#q=%s&page=%d' % (urllib.quote(query), page)\n",
    "r = s.get(url)\n",
    "encoding = r.encoding if 'charset' in r.headers.get('content-type', '').lower() else None\n",
    "soup = BeautifulSoup(r.content, 'lxml', from_encoding = encoding)\n",
    "print soup.title\n",
    "main_holder = soup.find('div', {'data-react-class' : 'Serp'})\n",
    "print main_holder\n",
    "for a in article_list:\n",
    "    print a.text\n",
    "    with open('./test/test.txt', 'a') as output_file:\n",
    "        output_file.write(a.text.encode('utf8') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
